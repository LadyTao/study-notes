flink job在运行时如何确保正常运行，是否有背压存在？如何解决背压？checkpoint是否正常，如果不正常怎么解决？如何调试flink程序，怎么能知道程序中哪一个/些函数调用占用太多资源开销？当状态后端成为限制吞吐量的因素，我们又如何解决？请看本章内容

#### 5.1 监控
flink web UI的监控界面提供两个重要的监控项目：checkpoint和back pressure。
**关于checkpoint：**

![image](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/Image%20%5B3%5D.png)

关于历史的检查点标签，我们可以发现以下数据：

* Status: 检查点的当前状态，可以是In Progress、Completed或Failed。

* Trigger Time: 触发检查点时在 JobManager 的时间。

* Latest Acknowledgement: 在 JobManager 收到任何子任务的最新确认的时间（如果尚未收到确认，则为 n/a）。

* End to End Duration: 从触发时间戳到最新确认的持续时间（如果尚未收到确认，则为 n/a）。完整检查点的端到端持续时间由确认检查点的最后一个子任务确定。这个时间通常大于单个子任务实际检查点状态所需的时间。

* Checkpointed Data Size: 所有已确认子任务的检查点数据大小。如果启用了增量检查点，则此值是检查点数据大小增量。

* Processed (persisted) in-flight data: 在所有已确认的子任务的对齐期间（接收第一个和最后一个检查点屏障之间的时间）处理/持久的大致字节数。仅当启用未对齐的检查点时，持久数据才可能大于零。

![image](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/Image%20%5B4%5D.png)

对于每一个checkpoint操作，可以查看更细的subtask的状态：

* Sync Duration: 检查点同步部分的持续时间。这包括operator的快照状态并阻止子任务上的所有其他活动（处理记录、触发计时器等）。

* Async Duration: 查点的异步部分的持续时间。这包括将检查点写入所选文件系统所花费的时间。对于未对齐的检查点，这还包括子任务必须等待最后一个检查点barrier到达的时间（对齐持续时间）以及持久化运行中数据所花费的时间。

* Alignment Duration: 处理第一个和最后一个检查点barrier之间的时间。对于对齐的检查点，在对齐过程中，已经收到检查点barrier的通道被阻止处理更多数据。

* Start Delay: 自创建检查点barrier以来，第一个检查点barrier到达此子任务所用的时间。

![image](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/Image%20%5B5%5D.png)

在checkpoint的configuration界面，可以发现以下信息：

* Checkpointing Mode: 检查点模式，恰好一次或至少一次。

* Interval: 配置的检查点间隔。在此间隔内触发检查点。

* Timeout: JobManager 取消检查点并触发新检查点的超时时间。

* Minimum Pause Between Checkpoints: 检查点之间所需的最小暂停。在一个检查点成功完成后，我们至少要等待这段时间才能触发下一个检查点，这可能会延迟常规间隔。

* Maximum Concurrent Checkpoints: 可以同时进行的最大检查点数。

* Persist Checkpoints Externally: 启用或禁用。如果启用，还会列出外部检查点的清理配置（删除或取消保留）。


**关于背压**
![image](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/back_pressure_job_graph.png)

* 空闲任务是蓝色的
* 完全背压的任务是黑色的
* 完全忙碌的任务是红色的

两者之间的所有值都表示为这三种颜色之间的阴影。

#### 5.2 Metric使用
除了在flinkUI上查看监控信息，我们还可以使用第三方的软件，将监控信息发送过去。比如说，使用Report/JMX/Graphite/InfluxDB/Prometheus等。
对于普罗米修斯，我们有推和拉两种模式，一般我们都是使用push的方式，由flink自己去推送数据给PromutheusGateWay。这种方式的好处在于，首先对于集群节点的频繁变动，无需Prometheus去修改配置地址；其次，由flink去推送数据设计上更合理，flink本身提供对外的推送服务。

对于PrometheusPushGateway 有如下配置示例：
> metrics.reporter.promgateway.class: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter
> metrics.reporter.promgateway.host: localhostmetrics.reporter.promgateway.port: 9091
> metrics.reporter.promgateway.jobName: myJob
> metrics.reporter.promgateway.randomJobNameSuffix: true
> metrics.reporter.promgateway.deleteOnShutdown: false
> metrics.reporter.promgateway.groupingKey: k1=v1;k2=v2
> metrics.reporter.promgateway.interval: 60 SECONDS

如果是perJob模式一个任务一个集群这种模式，需要在提交任务的参数中指定：
> flink run -d -yqu flink -m yarn-cluster \
> -nm test10 \
> -p 4 \
> -yD metrics.reporter.promgateway.jobName=test10\
> -yD metrics.reporter.promgateway.groupingKey="jobname=test10"\
> -c flink.streaming.FlinkStreamFlatMapDemo \
> /home/jason/bigdata/jar/flink-1.13.0-1.0-SNAPSHOT.jar

其实就是因为这种模式下，每个任务都是一个独立的环境，如果都用一个jobName，其他的任务数据也会推送到这里，造成结果数据混乱。

**✳延迟监控**
流式系统中延迟是一个非常重要的监控指标，也就是算子到算子之间计算延迟是多长。flink开启配置：metrics.latency.interval，就可以在metric中看到askManagerJobMetricGroup/operator_id/operator_subtask_index/latency指标，借此观察算子与算子之间的延迟。

#### 5.3 调试
我们通过监控，看板，可以发现job出现了哪些问题，比如说checkpoint频繁失败，背压过高。这一节就对几个常见的场景给出针对性的解决方案。

##### 5.3.1 调整检查点间隔
当我们的程序启动了检查点功能，有一个情况是检查点的完成时间超过了配置的检查点间隔，那么在该检查点完成前是不会触发下一次检查点的。当发生这种行为时，默认情况下一旦完成当前这个延迟的检查点，系统将立即触发下一个检查点。那么就会有这样一种情况，可能我们的场景中完成检查点需要的时间会频繁的超过这个时间间隔，系统会不断地获取检查点，且不停的启动新的检查点，为了防止这种情况，可以给程序定义启动两个检查点的间隔：
> StreamExecutionEnvironment.getCheckpointConfig().setMinPauseBetweenCheckpoints(mills)

也就是说，有了这个时间间隔后，即使你的checkpoint超时完成了，下一个检查点还是会等待一段时间再启动。

![image](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/checkpoint_tuning.svg)


##### 5.3.2 调整RocksDB
生生产环境我们一般都是首选RocksDB作为状态后端，以下是一些针对RocksDB的调优策略：
* 增量检查点
这是非常重要的一点，与完整的检查点相比，增量检查点可以显著节省时间，而几乎没有什么损失。

* RocksDB内存调整
RocksDB的性能与其可使用的内存有着直接的联系，没默认情况下，RocksDB 状态后端使用flink的托管内存作为它的buffer和缓存。`state.backend.rocksdb.memory.managed: true`。以下几个步骤都可以考虑作为调优手段：
(1)提升flink的托管内存大小，默认由一个百分比分数(0.4)控制，这经常是直接有效的。
(2)如果你的应用程序有非常多的状态且看到MemTable频繁刷新(写入端瓶颈)，但又实在没有多余内存可用，可以增加写入缓存区的内存比率：`state.backend.rocksdb.memory.write-buffer-ratio`

##### 5.4 资源规划
为了确保任务可以正常运行，我们在分配资源时需要考虑这些因素：
(1)确保任务在无恒定背压下工作，如果有恒定背压，任务一定会延迟；
(2)在无恒定背压的情况下预留额外的资源，需要考虑到重启等额外因素，需要追赶一部分延时；
(3)临时背压无需额外关注；
(4)某些操作会导致下游operator的负载激增，比如说大的窗口，窗口构建时下游无事可做，窗口结束时下游负载激增，需要考虑到下游是否可以稳定处理这种情况。

##### 5.5 压缩
检查点和保存点可以采用压缩方式（目前只支持snappy），提高性能。
> ExectionConfig = executionConfig = new ExecutionConfig();
> executionConfig.setUseSnapshotCompress(true);

##### 5.6 任务本地恢复
我们知道检查点会生成状态快照并写入分布式存储。每个TM会将本地的状态信息汇报给JM，由JM写入磁盘中。任务恢复时也需要各个TM从分布式系统中远程拉取状态数据，当状态比较大时，这个过程可能比较耗时。为了解决这个问题，引入了本地恢复的概念。

本地恢复就是在TM本地将状态保留一个副本，当恢复任务时不必要从远程拉去状态信息。这么做是因为大部分情况下我们都可以直接从原先的TM恢复任务。注意本地只是一个备份，你任然需要配置远程的地址，防止本地保存点信息不完整或丢失。这个功能目前只支持keyed state，有一定的限制。
> state.backend.local-recovery:true


