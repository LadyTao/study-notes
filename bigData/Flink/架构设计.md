<u>*摘抄自官方文档*</u>

#### 1. Flink程序结构
自然环境中的数据都可以看作是流式的，比如说Web服务器的事件数据，证券交易所的数据，工厂车间的传感器数据等等。但是当我们在处理数据时，通常是分为有界(bounded)或无界(unbounded)两种模型入手去做分析。这两种思路使得程序的设计方式也大为不同。
![images](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/bounded-unbounded.png)

`批处理` 是有界数据流处理的范例，这种模式可以让你在计算结果时输入整个数据集，你可以对整个数据集进行排序，统计，汇总。

`流处理` 是无界数据流处理的范例。从理论上讲，输入的数据永远不会停止，程序也不能停止。

Flink程序的基本单元是`流`和`转换`。所谓流是指处理的数据记录，而转换是将一个或多个流作为输入，输出一个或多个流。

![images](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/program_dataflow.svg)

上图展示的就是一个基本的Flink程序结构：

**Source数据源。** Flink目前的source大体分为4类：
* 本地集合source
* 基于文件的source
* 基于网络套接字的source
* 自定义的source（eg. kafka, jdbc...）

**Transformation。** 数据转换操作。包含：
Map/FlatMap/Filter/KeyBy/Reduce/Fold/Aggregations/Window/WindowAll/Union/Window Join/Split/Select等操作。

**Sink接收器。** 将转换后的数据作为发送的目的地。常见的是：
* 写入文件、打印出来
* 写入socket
* 自定义sink（eg. es,hadoop,jdbc...）

#### 2. Flink并行数据流
Flink程序在执行时，会先被映射成一个`Streaming DataFlow`。 一个Streaming DataFlow是由一组Stream 和 Transformation Operator组成的。在启动时以一个或多个Source Operator开始，结束于一个或多个Sink Operator。

![images](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/parallel_dataflow.svg)

Flink是分布式并行处理框架，上面我们说到程序基本单元是stream和operator，一个Stream可以被分成多个分区，即Stream Partition；一个operator也可以被分成多个子任务，也就是subtask。subtask之间彼此独立，在不同的线程/不同机器中执行。一个operator中subtask的数量，就是这个operator的并行度。一个任务中不同的operator可以有不同的并行度。

上图source的并行度是2。一个stream的并行度等于生成它的operator的并行度。数据在两个operator之间传递有两种模式：
（1）One to One 模式：维持数据的分区和排序属性。
（2）Redistributing模式：改变数据的分区属性，每个subtask会根据Transformation把数据发送到不同的subtask。例如，keyby会通过hashcode重新分区，broadcast和rebalance会重新随机分区。

#### 3. Task 与 Opreator Chain
Flink的操作称为operator，客户端提交作业的时候会对operator进行优化操作，将可以合并的operator进行合并，合并之后的operator称为operator chain。通俗理解就是由operator组成的执行链，每个执行链会在TaskManager上的一个独立线程中执行。

![images](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/tasks_chains.svg)

#### 4. 任务槽(task slot) 与槽共享(slot sharing)
![images](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/tasks_slots.svg)

每个TaskManager都是一个JVM进程，可以在其线程中执行一个或多个subtask。为了可以控制一个TaskManager可以执行多少task，通过插槽(slot)来实现。
**任务槽(task slot)** 每个task slot表示TM拥有资源的一个固定大小的子集。一般来讲，TM的slot数量与其CPu的核数相等，比如是8核CPU就设置8个slot。Flink会将内存划分到每一个slot中。下图中有2个TM，每个TM有3个slot，每个slot占1/3的内存。内存在被划分到不同slot之后的好处是：
* TM最多能同时处理的任务是可以控制的，那就是slot的数量。任务槽的作用就是分离任务的托管内存，注意CPU不会隔离，从这里也能发现，session模式下，多个任务是共享CPU使用的，任务之间会彼此影响资源的使用。
* slot独占内存空间。TM中的多个作业，内存的使用不会彼此影响。

**槽共享** 
默认情况下，Flink允许subtask共享插槽，但是必须是同一个job的subtask。这样做的好处是：
* job当中最高并行度的subtask能满足资源的需求，那其他subtask也可以正常启动
* 资源分配更公平。如果有比较空闲的slot，就可以把更多的task分配给它。像下图，如果没有槽共享，负载不高的source/map等subtask会占据很多资源，造成浪费。通过槽共享，可以将并行度从2提升到6，充分利用了资源（与上面的图对比看）。
* 并行度间接提高。

![images](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/slot_sharing.svg)

#### 5. 任务调度与执行
![images](https://github.com/LadyTao/study-notes/blob/main/bigData/Flink/picture/processes.svg)

Flink执行流程如下：
1. 用户提交任务，客户端执行main方法(非application mode)，生成flink可以识别的DAG数据流图，即jobGraph；
2. ActorSystem创建Actor将jobGraph发送给JM中的Actor；
3. JM接收TM的心跳，获取TM的状态；
4. JM通过调度器在TM中调度执行task；
5. 运行过程中，task之间可以互传数据。

* **Job Client**
    主要任务就是提交job，提交后可以结束进程，也可以等待结果返回；它不是程序执行的内部部分，但它是job执行的起点；它接收用户代码，创建jobGraph并提交给JM进一步执行；执行完成后，job Client将结果返回给用户。
* **JobManager**
    负责调度task，协调checkpoint和容错；包含Actor System/Scheduler/Checkpoint三个组件；从客户端接收到jobGraph后，首先经过优化，再调度到TM执行。
    - ResourceManager: 负责集群资源的分配、释放，也就是槽位。
    - Dispacher：提供Rest接口提交job，并为每个job启动一个新的jobMaster，运行Web UI以提供相关job的执行情况。
    - JobMaster：管理单个jobGraph的执行，集群可以同时运行多个作业，每个作业都有自己的jobMaster实例。
* **TaskManager**
    从JM接收任务，部署并启动，接受上游的数据并处理；TM创建之初就设置好了slot数量，每个slot可以执行一个任务。

